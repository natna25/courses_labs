{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2'), \"upgrade TensorFlow version to TensorFlow 2\"\n",
    "from skimage.io import imread\n",
    "\n",
    "from utils import show, show_multiple\n",
    "from utils import get_splitted_data_with_size, plot_model_history, scale_data, apply_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip the data on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/cat/'):\n",
    "    print('Extracting cat image files...')\n",
    "    zf = zipfile.ZipFile('data/cat.zip')\n",
    "    zf.extractall('data/')\n",
    "if not os.path.exists('data/dog/'):\n",
    "    print('Extracting dog image files...')\n",
    "    zf = zipfile.ZipFile('data/dog.zip')\n",
    "    zf.extractall('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Introduction to Tensorflow and convolution filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/standard_vs_depthwise_conv.png\" style=\"width: 850px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Left: standard convolution, the whole kernel is parsing the input tensor for each output channel dimension**\n",
    "\n",
    "**- Right: depthwise convolution, each slide of the kernel is parsing each input dimension. The result is constructed afterward using a concatenation of the feature maps. That is particularly useful to retrieve a valid RGB image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample image example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = imread(\"data/panda.jpg\")\n",
    "sample_image = tf.cast(sample_image, tf.float32)  # we will cast it as a tensorflow EagerTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(sample_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - A) Simple box blur kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2d convolution with tensorflow:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2d(x, k, strides, padding, conv_type):\n",
    "    if conv_type == 'depthwise':\n",
    "        return tf.nn.depthwise_conv2d(\n",
    "            x, k, strides=strides, padding=padding\n",
    "        )\n",
    "    elif conv_type == 'standard':\n",
    "        return tf.nn.conv2d(\n",
    "            x, k, strides=strides, padding=padding\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kernel(kernel):\n",
    "    # move the channel dimension to the first one\n",
    "    # this way, it is easier to see the spacial organization of the kernel with print\n",
    "    print(np.transpose(kernel, (2, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_data = np.ones(shape=(5, 5, 3)).astype(np.float32)\n",
    "kernel_data = tf.divide(kernel_data, tf.reduce_sum(kernel_data, axis=[0, 1]))\n",
    "visualize_kernel(kernel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch_expanded = tf.expand_dims(sample_image, axis=0)\n",
    "kernel_data_expanded = tf.expand_dims(kernel_data, axis=-1)\n",
    "print('Kernel shape: %s' % str(kernel_data_expanded.shape))\n",
    "output_image = conv_2d(\n",
    "    image_batch_expanded, \n",
    "    kernel_data_expanded, \n",
    "    strides=(1, 1, 1, 1), \n",
    "    padding='SAME', conv_type='depthwise'\n",
    ")\n",
    "feature_map = output_image.numpy()\n",
    "show(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- Explain what happened here: what transformation has been applied to the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - B) Identity kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_data = np.zeros(shape=(3, 3, 3)).astype(np.float32)\n",
    "kernel_data[1, 1, :] = 1\n",
    "visualize_kernel(kernel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch_expanded = np.expand_dims(sample_image, axis=0)\n",
    "kernel_data_expanded = np.expand_dims(kernel_data, axis=-1)\n",
    "features_map = []\n",
    "for padding, strides in [\n",
    "    ('SAME', (1, 1, 1, 1)), ('VALID', (1, 1, 1, 1)), ('SAME', (1, 10, 10, 1))\n",
    "]:\n",
    "    feature_map = conv_2d(\n",
    "        image_batch_expanded, kernel_data_expanded, \n",
    "        strides=strides, \n",
    "        padding=padding, \n",
    "        conv_type='depthwise'\n",
    "    )\n",
    "    features_map.append(feature_map.numpy())\n",
    "show_multiple(features_map, figsize=(16, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- Try to modify the strides and the padding type. What are the effects on the final output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - C) Line detection kernel on greyscale transformed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_sample_image = tf.expand_dims(tf.reduce_sum(sample_image, axis=2) / 3., axis=-1)\n",
    "show(grey_sample_image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "Try to implement a kernel that does line or edge detection:\n",
    "- https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "- https://en.wikipedia.org/wiki/Sobel_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a 3x3 edge detection kernel\n",
    "line_detection_kernel = np.asarray(\n",
    "    [\n",
    "        # TODO:\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.],\n",
    "        [0., 0., 0.]\n",
    "    ]\n",
    ")\n",
    "\n",
    "kernel_data = tf.expand_dims(line_detection_kernel, axis=-1)\n",
    "visualize_kernel(kernel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = tf.expand_dims(grey_sample_image, axis=0)\n",
    "kernel_expanded = tf.cast(tf.expand_dims(kernel_data, axis=-1), tf.float32)\n",
    "output_line_detection = conv_2d(\n",
    "    image_batch, kernel_expanded,\n",
    "    strides=(1, 1, 1, 1), \n",
    "    padding='SAME', \n",
    "    conv_type='standard'\n",
    ")\n",
    "show(output_line_detection.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - D) Max and average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "Now apply a Max Pooling and an Average Pooling operations on our image.<br/>\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/max_pool\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool\n",
    "\n",
    "Again, try to make fluctuate the `ksize` and `strides` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch_expanded = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "# TODO:\n",
    "output_max_pool = None\n",
    "output_avg_pool = None\n",
    "\n",
    "if output_max_pool is not None and output_avg_pool is not None:\n",
    "    show_multiple([output_max_pool.numpy(), output_avg_pool.numpy()], figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Training a ConvNet with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - A) Load, resize and scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is advised to fix a relatively small image_size, for instance (32, 32, 3), to avoid suffering from slow calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (32, 32, 3)\n",
    "\n",
    "classes = ['cat', 'dog']\n",
    "X_tr, X_val, Y_tr, Y_val = get_splitted_data_with_size(\n",
    "    image_size=image_size, sample_size=10000, test_ratio=0.25, classes=classes, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape, X_val.shape, Y_tr.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(len(X_tr))\n",
    "show(X_tr[i])\n",
    "print('True label: {0}'.format(classes[Y_tr[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_scaled, X_val_scaled, scaler = scale_data(X_tr, X_val, return_scaler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - B) Design and train a ConvNet from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "- Implement a Convolutional Network using the Keras Sequential API\n",
    "- Typically, you would use series of convolutional blocs: \n",
    "\n",
    "`\n",
    "model.add(Conv2D(output_filter, (kernel_height, kernel_width), padding, input_shape=(input_height, input_width, input_filter)))\n",
    "model.add(Activation(activation))\n",
    "model.add(BatchNormalization())\n",
    "`\n",
    "- Usually, the output_filter size grows accross the network\n",
    "- End the network with a `Flatten` layer followed by a final `Dense` layer\n",
    "- Be careful with the shapes accross the network, the activation functions used, the optimizer, and the loss function\n",
    "- Don't forget to use Dropout layers to avoid overfitting issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\"\"\"\n",
    "Return a compiled Keras model\n",
    "\"\"\"\n",
    "def design_and_compile_model():\n",
    "    model = Sequential()\n",
    "    # TODO:\n",
    "    \n",
    "    return None\n",
    "\n",
    "    # Compiling the model adds a loss function, optimiser and metrics to track during training\n",
    "    model.compile(\n",
    "        optimizer=None,\n",
    "        loss=None,\n",
    "        metrics=None\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_and_compile_model().summary() if design_and_compile_model() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 20  # The number of epochs (full passes through the data) to train for\n",
    "\n",
    "model = design_and_compile_model()\n",
    "\n",
    "# The fit function allows you to fit the compiled model to some training data\n",
    "if model:\n",
    "    model_history = model.fit(\n",
    "        x=X_tr_scaled, \n",
    "        y=Y_tr, \n",
    "        batch_size=batch_size, \n",
    "        epochs=num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val_scaled, Y_val)\n",
    "    )\n",
    "    print('Training complete')\n",
    "else:\n",
    "    model_history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model_history) if model_history else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - C) Improve it using data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "- Try to improve the effectiveness of your network using some Data Augmentation\n",
    "- Basically, it consists in building a `ImageDataGenerator` fitted on your training dataset\n",
    "- Then you will be able to generate new consistent samples, and refit your model using the `fit_generator` Keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Instantiate a ImageDataGenerator object with the right parameters and then fit it on your training dataset\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = design_and_compile_model()\n",
    "# Fit your model with model.fit_generator() and feed it with data_generator.flow()\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model_history) if model_history else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Transfer learning\n",
    "\n",
    "Objectives:\n",
    "- Classify an image by loading a pre-trained ResNet50 model using Keras Zoo\n",
    "    - No training required\n",
    "    - Decode an ImageNet prediction\n",
    "- Build a headless model and compute representations of images \n",
    "    - Retrain a model from representations of images for your own classification task: here cat vs dog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sample_path = \"data/cat/cat_1.jpg\"\n",
    "dog_sample_path = \"data/dog/dog_1.jpg\"\n",
    "resnet_input_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "model_ResNet50 = ResNet50(include_top=True, weights='imagenet')\n",
    "model_ResNet50.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - A) Classify of an image using pre-trained weights\n",
    "\n",
    "**Exercise**\n",
    "- Open an image, preprocess it and build a batch of 1 image\n",
    "- Use the model to classify this image\n",
    "- Decode the predictions using `decode_predictions` from Keras\n",
    "\n",
    "Notes:\n",
    "- You may use `preprocess_input` for preprocessing the image. \n",
    "- Test your code with `\"data/cat/cat_1.jpg\"` \n",
    "- ResNet has been trained on (width, height) images of (224,224) and range of pixel intensities in `[0, 255]`.\n",
    "    - [skimage.transform.resize](http://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize) has a `preserve_range` keyword useful in that matter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread(cat_sample_path)\n",
    "img_resized = resize(img, resnet_input_size, mode='reflect', preserve_range=True)\n",
    "show(img_resized)\n",
    "\n",
    "# Use preprocess_input() to apply the same preprocessing as ResNet, \n",
    "# get the prediction from the loaded model, and then decode the predictions\n",
    "\n",
    "# TODO:\n",
    "decoded_predictions = None\n",
    "\n",
    "if decoded_predictions:\n",
    "    for _, name, score in decoded_predictions:\n",
    "        print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - B) Build a headless model and compute representations of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model has $177$ layers\n",
    "- See where we should stop to have the extracted feature and start building a new classficlation model from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_ResNet50.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's remove the last dense classification layer that is specific \n",
    "to the image net classes and use the previous layers (after flattening) as a feature extractors\n",
    "- Use ResNet input layer and last layer of extracted features to build a feature extractor model\n",
    "    - Use Keras functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a truncated Model using ResNet50.input and the before last layer\n",
    "\n",
    "# TODO:\n",
    "feat_extractor_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this model we need to be careful to apply the same image processing as was used during the training, otherwise the marginal distribution of the input pixels might not be on the right scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_resnet(x, size):\n",
    "    x = resize(x, size, mode='reflect', preserve_range=True)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    if x.ndim == 3:\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    return preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model extracts high level concepts from any image that has been preprocessed like the images ResNet trained on.\n",
    "The model transforms a preprocessed (224, 224) RGB image into a long vector of activations.\n",
    "Each activation refers to some concept statistically connected to a bunch of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img = imread(cat_sample_path)\n",
    "cat_img_processed = preprocess_resnet(cat_img, resnet_input_size)\n",
    "if feat_extractor_model:\n",
    "    cat_representation = feat_extractor_model.predict(cat_img_processed)\n",
    "    print(\"Cat deep representation shape: (%d, %d)\" % cat_representation.shape)\n",
    "    cat_representation_df = pd.DataFrame(np.ravel(cat_representation), columns=['cat_deep_features'])\n",
    "else:\n",
    "    cat_representation_df = None\n",
    "cat_representation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- What is the number of $0$s in the cat representation vector?\n",
    "- Can you find any negative values?\n",
    "- Why are there $0$ values? What does it mean?\n",
    "- Extract ResNet representations of other dogs and cats. Are the zeros at the same places in vector? Explain why or give an intuition of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feat_extractor_model:\n",
    "    plt.hist(np.where(cat_representation == 0)[1], bins=25)\n",
    "    plt.title(\"cat zeros positions\")\n",
    "    plt.show()\n",
    "\n",
    "    dog_img = imread(dog_sample_path)\n",
    "    dog_img_processed = preprocess_resnet(dog_img, resnet_input_size)\n",
    "    dog_representation = feat_extractor_model.predict(dog_img_processed)\n",
    "\n",
    "    plt.hist(np.where(dog_representation == 0)[1], bins=25)\n",
    "    plt.title(\"dog zeros positions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - C) Retrain a model from computed representations of images\n",
    "\n",
    "For this session we are going to use the dataset of the dogs-vs-cats we already used in part $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cat', 'dog']\n",
    "X_tr, X_val, Y_tr, Y_val = get_splitted_data_with_size(\n",
    "    image_size=(224, 224, 3), sample_size=2000, test_ratio=0.25, classes=classes, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- Inference time takes a long time only for $2000$ images. Explain why it would be much faster using a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feat_extractor_model:\n",
    "    X_extracted_tr = feat_extractor_model.predict(preprocess_input(X_tr), verbose=1)\n",
    "    X_extracted_val = feat_extractor_model.predict(preprocess_input(X_val), verbose=1)\n",
    "    print('Done extracting resnet50 features..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a classification model fed with the newly created X and Y. <br>Remember that X is now a set of ResNet representations of the images\n",
    "- Use either functional of sequential Keras apis\n",
    "- Display training and validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# TODO:\n",
    "transfer_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "- How high in validation accuracy did you get? \n",
    "- Compare to your previous classification model in part 2. Does it perform worse? Better? Why?\n",
    "- Did you observe overfitting during training? Why? If yes, what did you do to avoid it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
