{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from os import path\n",
    "from collections import Counter\n",
    "from urllib.request import urlretrieve\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "EPSILON = 1e-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Loading and visualizing pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two 100 dimensional pre-trained word embeddings are available here:\n",
    "- GloVe\n",
    "- Word2vec\n",
    "\n",
    "To measure similarity between vectors, we often use the cosine similarity.\n",
    "(https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "$cosine\\_similarity(w_1, w_2) = \\Large \\frac{\\langle w_1, w_2 \\rangle}{||w_1|| \\cdot ||w_2||} = \\frac{w_1^T \\cdot w_2}{||w_1|| \\cdot ||w_2||}$\n",
    "\n",
    "$w_1$ and $w_2$ are two word vector embeddings.\n",
    "\n",
    "It is a measure of how aligned $w_1$ and $w_2$ are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbeddings():\n",
    "    def __init__(self, language, embeddings):\n",
    "        self.vec_file = None\n",
    "        if language == 'en':\n",
    "            if embeddings == 'glove':\n",
    "                self.vec_file = 'glove_100k.en.vec.zip'\n",
    "            elif embeddings == 'w2v':\n",
    "                self.vec_file = 'w2v_1600k.en.vec.zip'\n",
    "        elif language == 'fr':\n",
    "            if embeddings == 'glove':\n",
    "                print('No GloVe french embeddings!')\n",
    "                return None\n",
    "            elif embeddings == 'w2v':\n",
    "                self.vec_file = 'w2v_800k.fr.vec.zip'\n",
    "        self.language = language\n",
    "        self.url = \"https://github.com/Deep-Learning-courses/courses_labs/releases/download/0.1/\" + self.vec_file\n",
    "        self.file_location_compressed = os.path.join('data', self.vec_file)\n",
    "        self.file_location = self.file_location_compressed.replace('.zip', '')\n",
    "        self.embeddings_index = None\n",
    "        self.embeddings_index_inversed = None\n",
    "        self.embeddings_vectors = None\n",
    "        self.voc_size = None\n",
    "        self.dim = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalize(array):\n",
    "        return array / np.linalg.norm(array, axis=-1, keepdims=True)\n",
    "        \n",
    "    def download_and_extract(self):\n",
    "        if not path.exists(self.file_location):\n",
    "            print('Downloading from %s to %s...' % (self.url, self.file_location_compressed))\n",
    "            urlretrieve(self.url, self.file_location_compressed)\n",
    "            print('Downloaded embeddings')\n",
    "            print('Extracting from %s to %s...' % (self.file_location_compressed, self.file_location))\n",
    "            with zipfile.ZipFile(self.file_location_compressed, 'r') as zip_ref:\n",
    "                zip_ref.extractall('data')\n",
    "            os.remove(self.file_location_compressed)\n",
    "    \n",
    "    \"\"\"\n",
    "    Note that you can choose to normalize directly the embeddings \n",
    "    to make the cosine similarity computation easier afterward.\n",
    "    \"\"\"\n",
    "    def load(self, normalize=False):\n",
    "        self.embeddings_index, self.embeddings_index_inversed = {}, {}\n",
    "        self.embeddings_vectors = []\n",
    "        file = open(self.file_location, encoding='utf-8')\n",
    "        header = next(file)\n",
    "        self.voc_size, self.dim = [int(token) for token in header.split()]\n",
    "        print('Vocabulary size: {0}\\nEmbeddings dimension: {1}'.format(self.voc_size, self.dim))\n",
    "        print('Loading embeddings in memory...')\n",
    "        for idx, line in enumerate(file):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            self.embeddings_index[word] = idx\n",
    "            self.embeddings_index_inversed[idx] = word\n",
    "            self.embeddings_vectors.append(vector)\n",
    "        self.embeddings_vectors = np.asarray(self.embeddings_vectors)\n",
    "        print('Embeddings loaded')\n",
    "        if normalize:\n",
    "            self.embeddings_vectors = self._normalize(self.embeddings_vectors)\n",
    "            print('Embeddings normalized')\n",
    "        file.close()\n",
    "    \n",
    "    \"\"\" \n",
    "    Return an embedding vector associated to a given word.\n",
    "    For this you are supposed to used the objects defined in the load function.\n",
    "    Be sure to handle the case where the received word is not found in the embeddings' vocabulary.\n",
    "    \"\"\"\n",
    "    def word_to_vec(self, word):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the closest word associated to a given embedding vector\n",
    "    The vector passed as argument might not be in self.embeddings_vectors\n",
    "    In other terms, you have to compute every cosine similarity between the vec argument\n",
    "    and the embeddings found in self.embeddings_vectors. Then determine the embedding in \n",
    "    self.embeddings_vectors with the highest similarity and return its associated string word\n",
    "    \"\"\"\n",
    "    def vec_to_closest_word(self, vec):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the n top similar words from a given string input.\n",
    "    The similarities are based on the cosine similarities between the embeddings vectors.\n",
    "    Note that the string could be a full sentence composed of several words.\n",
    "    Split the sentence, map the words that can be found in self.embeddings_vectors to vectors and\n",
    "    average them. \n",
    "    Then return the top (default: top=10) words associated to the top embeddings\n",
    "    in self.embeddings_vectors that have the highest cosine similarity with the previously computed average.\n",
    "    \"\"\"\n",
    "    def most_similar(self, query, top=10):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    def project_and_visualize(self, sample=1000):\n",
    "        embeddings_tsne = TSNE(perplexity=30).fit_transform(self.embeddings_vectors[:sample])\n",
    "        plt.figure(figsize=(40, 40))\n",
    "        axis = plt.gca()\n",
    "        np.set_printoptions(suppress=True)\n",
    "        plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], marker=\".\", s=1e-6)\n",
    "        for idx in range(sample):\n",
    "            plt.annotate(\n",
    "                self.embeddings_index_inversed[idx],\n",
    "                xy=(embeddings_tsne[idx, 0], embeddings_tsne[idx, 1]),\n",
    "                xytext=(0, 0), textcoords='offset points'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = PretrainedEmbeddings(language='en', embeddings='glove')\n",
    "pretrained_embeddings.download_and_extract()\n",
    "pretrained_embeddings.load(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.project_and_visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.vec_to_closest_word(pretrained_embeddings.word_to_vec('language'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.most_similar('beef and chicken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "#### • Observe the visualization plot. <br> &rarr; What can you say about very common words? <br> &rarr; Are you able to identify some clusters catching a specific semantical concept?\n",
    "\n",
    "#### • What may be happen if you try to call the \"most similar\" method with a very long sentence? Will it be accurate? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling random text from the model\n",
    "\n",
    "First part of language modelling will be about predicting the next character of a finite sequence of characters of size $k$.\n",
    "\n",
    "Recursively generate one character at a time: your model outputs the probability distribution $p_{\\theta}(c_{n} | c_{n-1}, \\ldots, c_{n-k})$\n",
    "\n",
    "Using this probability distribution, a predicted character $c_{n}$ will be sampled. The temperature parameter makes it possible to remove additional entropy (bias) into the parameterized multinoulli distribution of the output of the model.\n",
    "\n",
    "Then use your prediction $c_{n}$ to compute $c_{n+1}$, your model outputs: $p_{\\theta}(c_{n+1} | c_{n}, \\ldots, c_{n-k+1})$, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel():\n",
    "    def __init__(self):\n",
    "        self.corpus_path = None\n",
    "        self.corpus = None\n",
    "        \n",
    "    def _get_contiguous_sample(self, corpus, size):\n",
    "        print(len(corpus), size)\n",
    "        index = np.random.randint(0, len(corpus) - size + 1)\n",
    "        return corpus[index:index+size]\n",
    "    \n",
    "    def load_data(self, corpus_path, proportion_to_keep=1):\n",
    "        assert 0 <= proportion_to_keep <= 1, \"proportion_to_keep should be between 0 and 1\"\n",
    "        self.corpus_path = os.path.join('data', corpus_path)\n",
    "        file = open(self.corpus_path)\n",
    "        entire_corpus = file.read()\n",
    "        self.corpus = unidecode.unidecode(\n",
    "            self._get_contiguous_sample(\n",
    "                entire_corpus, size=int(proportion_to_keep*len(entire_corpus))\n",
    "            ).lower().replace(\"\\n\", \" \")\n",
    "        )\n",
    "        print('Corpus length: {0} characters'.format(len(self.corpus)))\n",
    "        file.close()\n",
    "        \n",
    "    def plot_vocabulary_distribution(self):\n",
    "        counter = Counter(self.corpus)\n",
    "        chars, counts = zip(*counter.most_common())\n",
    "        indices = np.arange(len(counts))\n",
    "        plt.figure(figsize=(16, 5))\n",
    "        plt.bar(indices, counts, 0.8)\n",
    "        plt.xticks(indices, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "\n",
    "language_model = LanguageModel()\n",
    "language_model.load_data('lambada_cleaned_sample.txt', proportion_to_keep=1e-4)\n",
    "print('Loaded sample:\\n{0}'.format(language_model.corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.plot_vocabulary_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - a) Character-based language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring per-character perplexity\n",
    "\n",
    "To measure the quality of a language model we usually use the perplexity.\n",
    "(https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "Here is how it is defined:\n",
    "\n",
    "$$perplexity_\\theta = 2^{-\\frac{1}{n} \\sum_{i=1}^{n} log_2 (p_\\theta(c_i)^T\\cdot y_i)}$$\n",
    "$p_\\theta(c_i)$ is your predicted column vector of probabilities over the possible next characters for the $i^{th}$ sequence.\n",
    "$y_i$ is the one-hot encoding vector of the answer: the next character of the $i^{th}$ sequence.\n",
    "\n",
    "You just compute the average negative loglikelihood like you have done previously, only using a log2 logarithm. Then just perform a base $2$ exponentiation of the quantity just computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tensorflow.keras imports\n",
    "\"\"\"\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Character based langage model implementation\n",
    "corpus_path: path of the corpus to load\n",
    "proportion_to_keep: contiguous proportion of the corpus that will be stored in memory\n",
    "hidden_size: hidden layer dimension\n",
    "char_embedding: if set to True, a dense embedding layer will be trained\n",
    "embedding_size: embedding layer dimension if char_embedding is True\n",
    "model: pre-trained model to use\n",
    "\"\"\"\n",
    "\n",
    "class CharLanguageModel(LanguageModel):\n",
    "    def __init__(self, corpus_path, proportion_to_keep=1,\n",
    "                 hidden_size=256, char_embedding=False, embedding_size=32, model=None):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.char_index = None\n",
    "        self.char_index_inversed = None\n",
    "        self.vocabulary_size = None\n",
    "        self.max_length_sequence = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.hidden_size = hidden_size\n",
    "        self.char_embedding = char_embedding\n",
    "        self.embedding_size = embedding_size\n",
    "        self.load_data(corpus_path, proportion_to_keep)\n",
    "        self.extract_vocabulary()\n",
    "        self.build_dataset()\n",
    "        self.model = self._get_model() if model is None else model\n",
    "        \n",
    "    \"\"\"\n",
    "    Define and compile a tensorflow.keras model, it will deal with sequences of length max_length_sequence\n",
    "    The representation size depends on the tokens' representation you chose\n",
    "    It should be composed of :\n",
    "        - an optional Embedding layer projecting into embedding_size dimensions: it will learn a distributed\n",
    "        representation of the given vocabulary\n",
    "        - one recurrent LSTM layer projecting into hidden_size dimensions\n",
    "        - one Dense layer with a softmax activation projecting into vocabulary_size dimensions\n",
    "    \"\"\"\n",
    "    def _get_model(self):\n",
    "        representation_size = self.embedding_size if self.char_embedding else self.vocabulary_size\n",
    "        # TODO:\n",
    "        return None\n",
    "\n",
    "    def extract_vocabulary(self):\n",
    "        chars = sorted(set(self.corpus))\n",
    "        self.char_index = dict((c, i) for i, c in enumerate(chars))\n",
    "        self.char_index_inversed = dict((i, c) for i, c in enumerate(chars))\n",
    "        self.vocabulary_size = len(self.char_index)\n",
    "        print('Vocabulary size: {0}'.format(self.vocabulary_size))\n",
    "        \n",
    "    \"\"\"\n",
    "    Methods to convert X into an encoded matrix\n",
    "    \n",
    "    Importante note: if the sequence length if smaller than max_length_sequence, \n",
    "    we pad the input with zeros vectors at the beginning of the encoded matrix\n",
    "    \n",
    "    Hints:\n",
    "      - You can use the sequence method from tensorflow.keras.preprocessing to pad the received sequences\n",
    "      - You can use the to_categorical method from tensorflow.keras.utils with a specified num_classes parameters to \n",
    "        one-hot encode your data\n",
    "    \"\"\"\n",
    "    def _integer_encoding(self, X):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    def _one_hot_encoding(self, X):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "    The matrices X and y are created in this method\n",
    "    It consists of sampling contiguous chunks in the corpus as training vectors with the next character as target\n",
    "    Then, an encoding method is called depending on the tokens' representation you chose to use in your model\n",
    "    \"\"\"\n",
    "    def build_dataset(self, min_length_sequence=5, max_length_sequence=30, step=10, trunc_freq=0.05):\n",
    "        self.X, self.y = [], []\n",
    "\n",
    "        for i in range(0, len(self.corpus)-max_length_sequence, step):\n",
    "            length_sequence = max_length_sequence\n",
    "            if np.random.rand() < trunc_freq:\n",
    "                length_sequence = np.random.choice(range(min_length_sequence, max_length_sequence+1))\n",
    "            self.X.append(self.corpus[i:i+length_sequence])\n",
    "            y_one_hot = np.zeros(self.vocabulary_size)\n",
    "            y_one_hot[self.char_index[self.corpus[i+length_sequence]]] = 1\n",
    "            self.y.append(y_one_hot)\n",
    "\n",
    "        self.X, self.y = np.asarray(self.X), np.asarray(self.y)\n",
    "\n",
    "        self.max_length_sequence = max_length_sequence\n",
    "        self.X, self.y = sklearn.utils.shuffle(self.X, self.y)\n",
    "        print('Number of training sequences: {0}'.format(len(self.X)))\n",
    "        if self.char_embedding:\n",
    "            self.X = self._integer_encoding(self.X)\n",
    "        else:\n",
    "            self.X = self._one_hot_encoding(self.X)\n",
    "        if self.X is not None and self.y is not None:\n",
    "            print('X shape: {0}\\ny shape: {1}'.format(self.X.shape, self.y.shape))\n",
    "\n",
    "    \"\"\"\n",
    "    Actual model training, call the model fit method\n",
    "    \"\"\"\n",
    "    def train(self, batch_size=128, epochs=10):\n",
    "        self.model.fit(\n",
    "            self.X, self.y, validation_split=0.1, batch_size=batch_size, epochs=epochs\n",
    "        )\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the prediction of our model, meaning the next token given an input sequence\n",
    "    \n",
    "    If preprocessed is specified as True, we consider X as an array of strings and we will transform\n",
    "    it to an encoded matrix\n",
    "    Else, if preprocessed is specified as False, we apply the model predict on X as it is\n",
    "    \"\"\"\n",
    "    def predict(self, X, verbose=1, preprocessed=True):\n",
    "        if not preprocessed:\n",
    "            X = self._integer_encoding(X) if self.char_embedding else self._one_hot_encoding(X)\n",
    "        return self.model.predict(X, verbose=verbose)\n",
    "    \n",
    "    \"\"\"\n",
    "    Perplexity metric used to appreciate the performance of our model\n",
    "    \"\"\"\n",
    "    def perplexity(self, y_true, y_pred):\n",
    "        likelihoods = np.sum(y_pred * y_true, axis=1)\n",
    "        return 2 ** -np.mean(np.log2(likelihoods + EPSILON))\n",
    "    \n",
    "    \"\"\"\n",
    "    Sample the next character according to the predictions.\n",
    "    \n",
    "    Use a lower temperature to force the model to output more\n",
    "    confident predictions: more peaky distribution.\n",
    "    \"\"\"\n",
    "    def _sample_next_char(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds + EPSILON) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds + EPSILON)\n",
    "        probs = np.random.multinomial(1, preds, size=1)\n",
    "        return np.argmax(probs)\n",
    "    \n",
    "    \"\"\"\n",
    "    Try to generate some text using the trained model and a given seed string as starting point\n",
    "    The process is to sample the next char in our predicted tokens' distribution \n",
    "    until we reach the specified length\n",
    "    \"\"\"\n",
    "    def generate_text(self, seed_string, length=300, temperature=1.0):\n",
    "        if self.model is None:\n",
    "            print('The language model has not been compiled yet!')\n",
    "            return seed_string\n",
    "        generated = seed_string\n",
    "        prefix = seed_string\n",
    "        for i in range(length):\n",
    "            predictions = np.ravel(self.predict([prefix], verbose=0, preprocessed=False))\n",
    "            next_index = self._sample_next_char(predictions, temperature)\n",
    "            next_char = self.char_index_inversed[next_index]\n",
    "            generated += next_char\n",
    "            prefix = prefix[1:] + next_char\n",
    "        return generated\n",
    "    \n",
    "    \"\"\"\n",
    "    Project and visualize our character embedding layer in 2D\n",
    "    \"\"\"\n",
    "    def visualize_char_embeddings(self):\n",
    "        if not self.char_embedding or self.model is None:\n",
    "            print('No trained character embedding layer!')\n",
    "            return None\n",
    "        char_embeddings = self.model.get_weights()[0]\n",
    "        embeddings_tsne = TSNE(perplexity=30).fit_transform(char_embeddings)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        axis = plt.gca()\n",
    "        np.set_printoptions(suppress=True)\n",
    "        plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], marker=\".\", s=1e-6)\n",
    "        for idx in range(len(char_embeddings)):\n",
    "            plt.annotate(\n",
    "                self.char_index_inversed[idx],\n",
    "                xy=(embeddings_tsne[idx, 0], embeddings_tsne[idx, 1]),\n",
    "                xytext=(0, 0), textcoords='offset points'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_for_debug = True\n",
    "proportion_to_keep = 0.01 if speedup_for_debug else 1\n",
    "language_model = CharLanguageModel(\n",
    "    corpus_path='lambada_cleaned_sample.txt', proportion_to_keep=proportion_to_keep, char_embedding=False\n",
    ")\n",
    "if language_model.model is not None:\n",
    "    language_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_string = \"go on, surprise me, say something clever to \"\n",
    "if language_model.model is not None:\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        language_model.train(epochs=1)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            perplexity = language_model.perplexity(\n",
    "                language_model.y, language_model.model.predict(language_model.X)\n",
    "            )\n",
    "            print('Perplexity after {} epochs: {}'.format(epoch + 1, perplexity))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.25))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.5))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(language_model.generate_text(seed_string, temperature=0.25) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.5) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.75) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### char embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_for_debug = True\n",
    "proportion_to_keep = 0.01 if speedup_for_debug else 1\n",
    "language_model = CharLanguageModel(\n",
    "    corpus_path='lambada_cleaned_sample.txt', proportion_to_keep=proportion_to_keep, char_embedding=True\n",
    ")\n",
    "if language_model.model is not None:\n",
    "    language_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_string = \"go on, surprise me, say something clever to \"\n",
    "if language_model.model is not None:\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        language_model.train(epochs=1)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            perplexity = language_model.perplexity(\n",
    "                language_model.y, language_model.model.predict(language_model.X)\n",
    "            )\n",
    "            print('Perplexity after {} epochs: {}'.format(epoch + 1, perplexity))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.25))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.5))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(language_model.generate_text(seed_string, temperature=0.25) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.5) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.75) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.visualize_char_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "#### • With a high temperature, out of vocabulary words seem to happen more often. Can you briefly explain why it is the case?\n",
    "#### • Adding an character embedding layer results to fewer trainable parameters in the end. <br> &rarr; Do the calculations and point out the main differences. <br> &rarr; Which one works better for text generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - b) Word-based language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Word based langage model implementation\n",
    "corpus_path: path of the corpus to load\n",
    "proportion_to_keep: contiguous proportion of the corpus that will be stored in memory\n",
    "max_vocabulary: maximum number of different words to keep, other will be replaced with <OUT> token\n",
    "hidden_size: hidden layer dimension\n",
    "pretrained_embeddings: instance of PretrainedEmbeddings containing pre-trained word embeddings\n",
    "embedding_size: embedding layer dimension, set to pretrained_embeddings.dim if pretrained_embeddings is specified\n",
    "model: pre-trained model to use\n",
    "\"\"\"\n",
    "\n",
    "class WordLanguageModel(LanguageModel):\n",
    "    def __init__(self, corpus_path, proportion_to_keep=1, max_vocabulary=1000,\n",
    "                 hidden_size=256, pretrained_embeddings=None, embedding_size=100,\n",
    "                 model=None):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.word_index = None\n",
    "        self.word_index_inversed = None\n",
    "        self.vocabulary_size = None\n",
    "        self.max_length_sequence = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.max_vocabulary = max_vocabulary\n",
    "        self.hidden_size = hidden_size\n",
    "        if pretrained_embeddings is not None:\n",
    "            if not isinstance(pretrained_embeddings, PretrainedEmbeddings):\n",
    "                print('pretrained_embeddings should be an instance of PretrainedEmbeddings class')\n",
    "                pretrained_embeddings = None\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.embedding_size = embedding_size if pretrained_embeddings is None else pretrained_embeddings.dim\n",
    "        self.load_data(corpus_path, proportion_to_keep)\n",
    "        self.punctuation_marks = None\n",
    "        self.punctuation_marks_inversed = None\n",
    "        self.corpus = self._tokenize(self.corpus)\n",
    "        self.allowed_words = None\n",
    "        self.corpus = self._limit_vocabulary(self.corpus)\n",
    "        self.extract_vocabulary()\n",
    "        self.build_dataset()\n",
    "        self.model = self._get_model() if model is None else model\n",
    "        if self.pretrained_embeddings is not None:\n",
    "            self._initialize_pretrained_embeddings()\n",
    "    \n",
    "    \"\"\"\n",
    "    Being a word based LM, we have to tokenize our corpus and \n",
    "    replace unremoved punctuation marks by specific tokens\n",
    "    \"\"\"\n",
    "    def _tokenize(self, corpus):\n",
    "        self.punctuation_marks = {\n",
    "            '.': ' <PERIOD> ',\n",
    "            ',': ' <COMMA> ',\n",
    "            '\\'': ' <QUOTE> ',\n",
    "            '?': ' <QUESTION> ',\n",
    "            '!': ' <EXCLAMATION> ',\n",
    "            ';': ' <SEMICOLON> ',\n",
    "            ':': ' <COLON> ',\n",
    "            '_': ' <UNDERSCORE> '\n",
    "        }\n",
    "        self.punctuation_marks_inversed = {v.strip(): k for k, v in self.punctuation_marks.items()}\n",
    "        for punctuation_mark, token in self.punctuation_marks.items():\n",
    "            corpus = corpus.replace(punctuation_mark, token)\n",
    "        return corpus.split()\n",
    "    \n",
    "    \"\"\"\n",
    "    To avoid out of memory issues, we can limit our corpus to have a vocabulary \n",
    "    with a maximum number of different words\n",
    "    Out of vocabulary words will be replaced by the <OUT> token\n",
    "    During the inference, we'll use the existing counter\n",
    "    \"\"\"\n",
    "    def _limit_vocabulary(self, corpus, use_existing_counter=False):\n",
    "        if not use_existing_counter:\n",
    "            counter = Counter(corpus)\n",
    "            self.allowed_words = set([item[0] for item in counter.most_common(self.max_vocabulary)])\n",
    "        return [word if word in self.allowed_words else '<OUT>' for word in corpus]\n",
    "    \n",
    "    \"\"\"\n",
    "    Define and compile a tensorflow.keras model, it will deal with sequences of length max_length_sequence\n",
    "    It should be composed of :\n",
    "        - an Embedding layer projecting into embedding_size dimensions: it will learn a distributed\n",
    "        representation of the given vocabulary\n",
    "        - one recurrent LSTM layer projecting into hidden_size dimensions\n",
    "        - one Dense layer with a softmax activation projecting into vocabulary_size dimensions\n",
    "    \"\"\"\n",
    "    def _get_model(self):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "    If a pretrained embeddings object is given, we overwrite the embedding weights matrix\n",
    "    by the received pretrained embeddings\n",
    "    \n",
    "    Importante note: all the words in the corpus probably do not exist in the pretrained embeddings object \n",
    "    (at least punctuation tokens we just replaced earlier). \n",
    "    To solve this, we have to construct a map of indexes from one matrix to another, \n",
    "    and overwrite only rows containing words that are existing in both\n",
    "    \n",
    "    Hints: \n",
    "    you can retrieve the weights of a layer using its get_weights method, and update them as a numpy matrix\n",
    "    then, you can plug them back in the layer using its set_weights method\n",
    "    \"\"\"\n",
    "    def _initialize_pretrained_embeddings(self):\n",
    "        index_to_pretrained_embeddings = {\n",
    "            index: self.pretrained_embeddings.embeddings_vectors[self.pretrained_embeddings.embeddings_index[word]] \n",
    "            for word, index in self.word_index.items()\n",
    "            if word in self.pretrained_embeddings.embeddings_index_inversed.values()\n",
    "        }\n",
    "        # TODO:\n",
    "\n",
    "    def extract_vocabulary(self):\n",
    "        words = sorted(set(self.corpus))\n",
    "        self.word_index = dict((c, i) for i, c in enumerate(words))\n",
    "        self.word_index_inversed = dict((i, c) for i, c in enumerate(words))\n",
    "        self.vocabulary_size = len(self.word_index)\n",
    "        print('Vocabulary size: {0}'.format(self.vocabulary_size))\n",
    "    \n",
    "    \"\"\"\n",
    "    Methods to convert X into an integer-encoded matrix\n",
    "    \n",
    "    One-hot encoding is discouraged in word-based language models: it would probably result to out of memory \n",
    "    issues due to a very large vocabulary size. Consequently, an embedding layer is mandatory\n",
    "    \n",
    "    Importante note: if the sequence length if smaller than max_length_sequence, \n",
    "    we pad the input with zeros vectors at the beginning of the encoded matrix\n",
    "    \n",
    "    Hints:\n",
    "      - You can use the sequence method from tensorflow.keras.preprocessing to pad the received sequences\n",
    "    \"\"\"\n",
    "    def _integer_encoding(self, X):\n",
    "        # TODO:\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "    The matrices X and y are created in this method\n",
    "    It consists of sampling contiguous chunks in the corpus as training vectors with the next word as target\n",
    "    Then, the integer encoding method is called on our design matrix\n",
    "    \"\"\"\n",
    "    def build_dataset(self, min_length_sequence=3, max_length_sequence=10, step=5, trunc_freq=0.05):\n",
    "        self.X, self.y = [], []\n",
    "\n",
    "        for i in range(0, len(self.corpus)-max_length_sequence, step):\n",
    "            length_sequence = max_length_sequence\n",
    "            if np.random.rand() < trunc_freq:\n",
    "                length_sequence = np.random.choice(range(min_length_sequence, max_length_sequence+1))\n",
    "            self.X.append(self.corpus[i:i+length_sequence])\n",
    "            y_one_hot = np.zeros(self.vocabulary_size)\n",
    "            y_one_hot[self.word_index[self.corpus[i+length_sequence]]] = 1\n",
    "            self.y.append(y_one_hot)\n",
    "\n",
    "        self.X, self.y = np.asarray(self.X), np.asarray(self.y)\n",
    "\n",
    "        self.max_length_sequence = max_length_sequence\n",
    "        self.X, self.y = sklearn.utils.shuffle(self.X, self.y)\n",
    "        print('Number of training sequences: {0}'.format(len(self.X)))\n",
    "        self.X = self._integer_encoding(self.X)\n",
    "        if self.X is not None and self.y is not None:\n",
    "            print('X shape: {0}\\ny shape: {1}'.format(self.X.shape, self.y.shape))\n",
    "        \n",
    "    \"\"\"\n",
    "    Actual model training, call the model's fit method\n",
    "    \"\"\"\n",
    "    def train(self, batch_size=128, epochs=10):\n",
    "        self.model.fit(\n",
    "            self.X, self.y, validation_split=0.1, batch_size=batch_size, epochs=epochs\n",
    "        )\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the prediction of our model, meaning the next token given an input sequence\n",
    "    \n",
    "    If preprocessed is specified as True, we consider X as an array of strings and we will transform\n",
    "    it to an encoded matrix\n",
    "    Else, if preprocessed is specified as False, we apply the model predict on X as it is\n",
    "    \"\"\"\n",
    "    def predict(self, X, verbose=1, preprocessed=True):\n",
    "        if not preprocessed:\n",
    "            X = self._integer_encoding(X)\n",
    "        return self.model.predict(X, verbose=verbose)\n",
    "    \n",
    "    \"\"\"\n",
    "    Perplexity metric used to appreciate the performance of our model\n",
    "    \"\"\"\n",
    "    def perplexity(self, y_true, y_pred):\n",
    "        likelihoods = np.sum(y_pred * y_true, axis=1)\n",
    "        return 2 ** -np.mean(np.log2(likelihoods + EPSILON))\n",
    "    \n",
    "    \"\"\"\n",
    "    Sample the next word according to the predictions.\n",
    "    \n",
    "    Use a lower temperature to force the model to output more\n",
    "    confident predictions: more peaky distribution.\n",
    "    \"\"\"\n",
    "    def _sample_next_word(self, preds, temperature=1.0):\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds[language_model.word_index['<OUT>']] = 0.  # force the model to avoid predicting <OUT>\n",
    "        preds = np.log(preds + EPSILON) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds + EPSILON)\n",
    "        probs = np.random.multinomial(1, preds, size=1)\n",
    "        return np.argmax(probs)\n",
    "    \n",
    "    def _post_processing(self, text):\n",
    "        text = text.replace(' . ', '. ').replace(' , ', ', ').replace(' \\' ', '\\'').replace(' ? ', '? ')\n",
    "        text = text.replace(' ! ', '! ').replace(' ; ', '; ').replace(' : ', ': ').replace(' _ ', '_')\n",
    "        return text\n",
    "    \n",
    "    \"\"\"\n",
    "    Try to generate some text using the trained model and a given seed string as starting point\n",
    "    The process is to sample the next char in our predicted tokens' distribution \n",
    "    until we reach the specified length\n",
    "    \"\"\"\n",
    "    def generate_text(self, seed_string, length=300, temperature=1.0):\n",
    "        if self.model is None:\n",
    "            print('The language model has not been compiled yet!')\n",
    "            return seed_string\n",
    "        seed_string = self._limit_vocabulary(self._tokenize(seed_string), use_existing_counter=True)\n",
    "        generated = seed_string\n",
    "        prefix = seed_string\n",
    "        for i in range(length):\n",
    "            predictions = np.ravel(self.predict([prefix], verbose=0, preprocessed=False))\n",
    "            next_index = self._sample_next_word(predictions, temperature)\n",
    "            next_word = self.word_index_inversed[next_index]\n",
    "            generated += [next_word]\n",
    "            prefix = prefix[1:] + [next_word]\n",
    "        generated = ' '.join([self.punctuation_marks_inversed.get(token, token) for token in generated])\n",
    "        return self._post_processing(generated)\n",
    "    \n",
    "    \"\"\"\n",
    "    Project and visualize our word embedding layer in 2D\n",
    "    \"\"\"\n",
    "    def visualize_word_embeddings(self):\n",
    "        if self.model is None:\n",
    "            print('The language model has not been compiled yet!')\n",
    "            return None\n",
    "        word_embeddings = self.model.get_weights()[0]\n",
    "        embeddings_tsne = TSNE(perplexity=30).fit_transform(word_embeddings)\n",
    "        plt.figure(figsize=(40, 40))\n",
    "        axis = plt.gca()\n",
    "        np.set_printoptions(suppress=True)\n",
    "        plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], marker=\".\", s=1e-6)\n",
    "        for idx in range(len(word_embeddings)):\n",
    "            plt.annotate(\n",
    "                self.word_index_inversed[idx],\n",
    "                xy=(embeddings_tsne[idx, 0], embeddings_tsne[idx, 1]),\n",
    "                xytext=(0, 0), textcoords='offset points'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_for_debug = True\n",
    "proportion_to_keep = 0.01 if speedup_for_debug else 1\n",
    "language_model = WordLanguageModel(\n",
    "    corpus_path='lambada_cleaned_sample.txt', \n",
    "    proportion_to_keep=proportion_to_keep, max_vocabulary=1000, pretrained_embeddings=None\n",
    ")\n",
    "if language_model.model is not None:\n",
    "    language_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_string = \"go on, surprise me, say something to \"\n",
    "if language_model.model is not None:\n",
    "    epochs = 20\n",
    "    for epoch in range(epochs):\n",
    "        language_model.train(epochs=1)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            perplexity = language_model.perplexity(\n",
    "                language_model.y, language_model.model.predict(language_model.X)\n",
    "            )\n",
    "            print('Perplexity after {} epochs: {}'.format(epoch + 1, perplexity))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.25))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.5))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(language_model.generate_text(seed_string, temperature=0.25) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.5) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.75) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_for_debug = True\n",
    "proportion_to_keep = 0.01 if speedup_for_debug else 1\n",
    "language_model = WordLanguageModel(\n",
    "    corpus_path='lambada_cleaned_sample.txt', \n",
    "    proportion_to_keep=proportion_to_keep, max_vocabulary=1000, pretrained_embeddings=pretrained_embeddings\n",
    ")\n",
    "if language_model.model is not None:\n",
    "    language_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_string = \"go on, surprise me, say something to \"\n",
    "if language_model.model is not None:\n",
    "    epochs = 20\n",
    "    for epoch in range(epochs):\n",
    "        language_model.train(epochs=1)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            perplexity = language_model.perplexity(\n",
    "                language_model.y, language_model.model.predict(language_model.X)\n",
    "            )\n",
    "            print('Perplexity after {} epochs: {}'.format(epoch + 1, perplexity))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.25))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.5))\n",
    "            print(language_model.generate_text(seed_string, temperature=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(language_model.generate_text(seed_string, temperature=0.25) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.5) + '\\n')\n",
    "print(language_model.generate_text(seed_string, temperature=0.75) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.visualize_word_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "#### • Explain the main differences with the previous character based language model.\n",
    "#### • Are the predicted sentences making more sense in this part? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "import utils\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - Introduction to PyTorch with classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will focus on building Deep Learning architectures for text classification using the PyTorch framework.\n",
    "\n",
    "To do so, we will use the IMDB dataset that has 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification.\n",
    "\n",
    "The goal is to classify the positive and negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to handle small subset and perform fast debug before actual traning\n",
    "quick_for_debug = True\n",
    "df = pd.read_csv(\n",
    "    \"data/movie_reviews.csv\", sep=\";\", nrows=100 if quick_for_debug else 1e6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class, **SentimentClassifier**, is an abstract class which you have to inherit from in the next $2$ subparts.\n",
    "\n",
    "This class forces you to implement the **forward** method that represents the neural net forward pass. Implementing this method is analogous to building a network with the tensorflow.keras functional API\n",
    "\n",
    "The constructor handles transforming most common words to indices, padding and truncating. After the constructor is done X is a LongTensor of shape $\\text{(nb_reviews, max_seq_length)}$. The elements along the $\\text{max_seq_length}$ axis are the word indices of a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(ABC, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        max_vocabulary_size=20000,\n",
    "        max_seq_length=100,\n",
    "        truncating=\"post\",\n",
    "        padding=\"post\",\n",
    "        embedding_dim=10,\n",
    "        learning_rate=0.01,\n",
    "        p_val=0.2,\n",
    "    ):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.X = dataset[\"message\"]\n",
    "        self.Y = dataset[\"label\"]\n",
    "        self.X_tr = None\n",
    "        self.X_val = None\n",
    "        self.Y_tr = None\n",
    "        self.Y_val = None\n",
    "        self._construct_dataset(\n",
    "            max_vocabulary_size, max_seq_length, truncating, padding, p_val\n",
    "        )\n",
    "        self.optimizer = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.BCELoss(reduction=\"mean\")\n",
    "        self.total_loss = None\n",
    "        self.total_acc = None\n",
    "        self.total_loss_nb_samples = None\n",
    "\n",
    "    def _construct_dataset(\n",
    "        self, max_vocabulary_size, max_seq_length, truncating, padding, p_val\n",
    "    ):\n",
    "        self.Y = torch.FloatTensor(self.Y).unsqueeze(1)\n",
    "        tokenizer = Tokenizer(num_words=max_vocabulary_size, char_level=False)\n",
    "        tokenizer.fit_on_texts(self.X)\n",
    "        self.X = tokenizer.texts_to_sequences(self.X)\n",
    "        self.X = pad_sequences(\n",
    "            self.X,\n",
    "            maxlen=max_seq_length,\n",
    "            value=0,\n",
    "            truncating=truncating,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.X = torch.LongTensor(self.X)\n",
    "        size_val = int(p_val * self.X.shape[0])\n",
    "        idxs = np.arange(self.X.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        idxs_tr = idxs[:-size_val]\n",
    "        idxs_val = idxs[-size_val:]\n",
    "        self.X_tr = self.X[idxs_tr]\n",
    "        self.X_val = self.X[idxs_val]\n",
    "        self.Y_tr = self.Y[idxs_tr]\n",
    "        self.Y_val = self.Y[idxs_val]\n",
    "\n",
    "    def _train_on_batch(self, input, target, return_metrics=True):\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.forward(input)\n",
    "        loss_on_batch = self.criterion(prediction, target)\n",
    "        loss_on_batch.backward()\n",
    "        self.optimizer.step()\n",
    "        prediction = (prediction > 0.5).type(torch.FloatTensor)\n",
    "        mean_acc = torch.eq(prediction, target).type(torch.FloatTensor).mean()\n",
    "        if return_metrics:\n",
    "            return loss_on_batch, mean_acc\n",
    "\n",
    "    def train(self, nb_epoch=10, batch_size=64):\n",
    "        if self.optimizer is None:\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        arr = np.arange(self.X_tr.size()[0])\n",
    "        np.random.shuffle(arr)\n",
    "\n",
    "        nb_batch = int(self.X_tr.size()[0] / batch_size)\n",
    "        verbose_every = 5 if nb_batch >= 5 else 1\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            self._reset_monitor_train_epoch()\n",
    "            if epoch > 0:\n",
    "                print()\n",
    "            for batch_idx in range(nb_batch):\n",
    "                idxs = arr[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "                X_batch_tr = self.X_tr[idxs]\n",
    "                Y_batch_tr = self.Y_tr[idxs]\n",
    "\n",
    "                mean_batch_loss_tr, mean_acc_tr = self._train_on_batch(\n",
    "                    X_batch_tr, Y_batch_tr\n",
    "                )\n",
    "                self._monitor_train_epoch(\n",
    "                    mean_batch_loss_tr, X_batch_tr.shape[0], mean_acc_tr\n",
    "                )\n",
    "\n",
    "                if (batch_idx + 1) % verbose_every == 0:\n",
    "                    self._display_training(\n",
    "                        epoch, nb_epoch, batch_idx, nb_batch, epoch_ended=False\n",
    "                    )\n",
    "\n",
    "            self._monitor_validation()\n",
    "            self._display_training(\n",
    "                epoch, nb_epoch, batch_idx, nb_batch, epoch_ended=True\n",
    "            )\n",
    "\n",
    "    def _monitor_train_epoch(self, mean_batch_loss, batch_size, mean_acc):\n",
    "        self.total_loss += mean_batch_loss * batch_size\n",
    "        self.total_acc += mean_acc * batch_size\n",
    "        self.total_loss_nb_samples += batch_size\n",
    "\n",
    "    def _reset_monitor_train_epoch(self):\n",
    "        self.total_loss = 0\n",
    "        self.total_loss_nb_samples = 0\n",
    "        self.total_acc = 0\n",
    "\n",
    "    def _monitor_validation(self):\n",
    "        prediction_val = self(self.X_val)\n",
    "        self.last_loss_val = self.criterion(prediction_val, self.Y_val)\n",
    "        prediction_val = (prediction_val > 0.5).type(torch.FloatTensor)\n",
    "        self.last_acc_val = (\n",
    "            torch.eq(prediction_val, self.Y_val).type(torch.FloatTensor).mean()\n",
    "        )\n",
    "\n",
    "    def _display_training(\n",
    "        self, epoch, nb_epoch, idx_batch, nb_batch, epoch_ended=False\n",
    "    ):\n",
    "        msg = \"Epoch {}/{} {} {} {}\".format(\n",
    "            epoch + 1,\n",
    "            nb_epoch,\n",
    "            utils.arrow(idx_batch + 1, nb_batch),\n",
    "            \" mean loss: %.5f\" % (self.total_loss.item() / self.total_loss_nb_samples),\n",
    "            \" mean acc : %.2f\" % (self.total_acc / self.total_loss_nb_samples),\n",
    "        )\n",
    "        if epoch_ended:\n",
    "            msg += \" val loss: {:.5f} val acc: {:.2f}\".format(\n",
    "                self.last_loss_val, self.last_acc_val\n",
    "            )\n",
    "        print(msg, end=\"\\r\")\n",
    "        # print(msg)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - a) Simple classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following computes a very simple model:\n",
    "<img src=\"../images/supervised_text_classification.png\" style=\"width: 600px;\" />\n",
    "Use the layers that can be found under torch.nn: https://pytorch.org/docs/stable/nn.html\n",
    "- torch.nn.Embedding: build an embedding layer mapping each word to a vector representation\n",
    "- torch.nn.Linear: end with a matrix product to set output at the right dimension (here $1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Finish to implement the constructor.\n",
    "    - Instantiate self.embedding, that should be able to encode \n",
    "    a vocabulary of self.max_vocabulary_size words vector at last \n",
    "    timestep to be used for classification\n",
    "    - The output of the embedding layer should be \n",
    "    (batch_size, max_seq_length, embedding_dim), compute the mean of the \n",
    "    tensor over the max_seq_length to produce a sentence context vector \n",
    "    to be used for classification\n",
    "    - Instantiate self.linear, a product matrix layer to be used over the \n",
    "    context vector to set the final output at the right dimension \n",
    "    hints:\n",
    "        - To make the output between 0 and 1, use the torch.sigmoid method\n",
    "\"\"\"\n",
    "class AverageSentimentClassifier(SentimentClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        max_vocabulary_size=20000,\n",
    "        max_seq_length=200,\n",
    "        truncating=\"post\",\n",
    "        padding=\"post\",\n",
    "        embedding_dim=10,\n",
    "        learning_rate=0.01,\n",
    "        p_val=0.2,\n",
    "    ):\n",
    "\n",
    "        super(AverageSentimentClassifier, self).__init__(\n",
    "            dataset,\n",
    "            max_vocabulary_size=max_vocabulary_size,\n",
    "            max_seq_length=max_seq_length,\n",
    "            truncating=truncating,\n",
    "            padding=padding,\n",
    "            embedding_dim=embedding_dim,\n",
    "            learning_rate=learning_rate,\n",
    "            p_val=p_val,\n",
    "        )\n",
    "\n",
    "        # TODO:\n",
    "        self.embedding = nn.Parameter(data=torch.zeros(1))\n",
    "        self.linear = nn.Parameter(data=torch.zeros(1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # cast float to int when random float numbers are via torchsummary\n",
    "        inputs = (\n",
    "            inputs.type(torch.LongTensor)\n",
    "            if not isinstance(inputs, torch.LongTensor)\n",
    "            else inputs\n",
    "        )\n",
    "\n",
    "        # TODO:\n",
    "        x = torch.zeros(inputs.shape[0], 1, requires_grad=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentiment_classifier = AverageSentimentClassifier(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    summary(avg_sentiment_classifier, (avg_sentiment_classifier.X_tr.shape[1],))\n",
    "except:\n",
    "    avg_sentiment_classifier = AverageSentimentClassifier(df)\n",
    "    print(avg_sentiment_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentiment_classifier.train(nb_epoch=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = avg_sentiment_classifier(avg_sentiment_classifier.X_tr[:1])\n",
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - b) Recurrent classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to enrich the previous model with recurrent LSTM layers:\n",
    "<img src=\"../images/unrolling_rnn_embeddings.png\" style=\"width: 600px;\" />\n",
    "Use the layers that can be found under torch.nn: https://pytorch.org/docs/stable/nn.html\n",
    "- torch.nn.Embedding: build an embedding layer mapping each word to a vector representation\n",
    "- torch.nn.LSTM: process batch of sequences of embedded words to batch of final hidden vectors\n",
    "- torch.nn.Linear: end with a matrix product to set output at the right dimension (here $1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: note that pytorch RNNs such as LSTM or GRU expect an input tensor of shape $\\text{(nb_timesteps, batch_size, hidden_dim)}$. After embedding in the forward pass, if the batch_size axis is in 1st position you have to switch it to the middle using the torch.Tensor.permute method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Finish to implement the constructor.\n",
    "    - Instantiate self.embedding, that should be able to encode \n",
    "    a vocabulary of self.max_vocabulary_size words\n",
    "    - Instantiate self.lstm, should go through all embedded words \n",
    "    in a sentence and produce a final hidden vector at last timestep \n",
    "    to be used for classification\n",
    "    - Instantiate self.linear, a product matrix layer to be used over \n",
    "    the LSTM output to set the final output at the right dimension \n",
    "    note:\n",
    "        - in forward method, input is of shape (batch_size, max_seq_length)\n",
    "        but torch.nn.LSTM layer expects an input of shape (max_seq_length, batch_size, embedding_dim).\n",
    "        Be careful about the position of the batch_size axis\n",
    "    hints:\n",
    "        - To make the output between 0 and 1, use the torch.sigmoid method\n",
    "\"\"\"\n",
    "class LstmSentimentClassifier(SentimentClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        max_vocabulary_size=20000,\n",
    "        max_seq_length=100,\n",
    "        truncating=\"post\",\n",
    "        padding=\"post\",\n",
    "        embedding_dim=10,\n",
    "        learning_rate=0.01,\n",
    "        p_val=0.2,\n",
    "        hidden_dim=32,\n",
    "    ):\n",
    "\n",
    "        super(LstmSentimentClassifier, self).__init__(\n",
    "            dataset,\n",
    "            max_vocabulary_size=max_vocabulary_size,\n",
    "            max_seq_length=max_seq_length,\n",
    "            truncating=truncating,\n",
    "            padding=padding,\n",
    "            embedding_dim=embedding_dim,\n",
    "            learning_rate=learning_rate,\n",
    "            p_val=p_val,\n",
    "        )\n",
    "\n",
    "        # TODO:\n",
    "        self.embedding = nn.Parameter(data=torch.zeros(1))\n",
    "        self.lstm = nn.Parameter(data=torch.zeros(1))\n",
    "        self.linear = nn.Parameter(data=torch.zeros(1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # cast float to int when random float numbers are via torchsummary\n",
    "        inputs = (\n",
    "            inputs.type(torch.LongTensor)\n",
    "            if not isinstance(inputs, torch.LongTensor)\n",
    "            else inputs\n",
    "        )\n",
    "\n",
    "        # TODO:\n",
    "        x = torch.zeros(inputs.shape[0], 1, requires_grad=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_sentiment_classifier = LstmSentimentClassifier(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just print model here, torchsummary.summary does not do well with lstm layer\n",
    "print(lstm_sentiment_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_sentiment_classifier.train(nb_epoch=3, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
